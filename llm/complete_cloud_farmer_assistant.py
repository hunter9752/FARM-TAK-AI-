#!/usr/bin/env python3
"""
Complete Cloud Farmer Assistant: STT â†’ NLP â†’ Cloud LLM + Real-time Data
Real-time speech with internet-connected intelligent responses
"""

import sys
import os
import json
import time
import threading
from datetime import datetime

# Add parent directories to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'stt vosk model'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'nlp'))

try:
    from improved_stt import ImprovedSTT
    from csv_based_intent_detector import CSVBasedFarmerIntentDetector
    from cloud_llm_assistant import CloudLLMFarmerAssistant
except ImportError as e:
    print(f"âŒ Import error: {e}")
    print("Please ensure all required modules are available.")
    sys.exit(1)


class CompleteCloudFarmerAssistant:
    """Complete pipeline: Speech â†’ Text â†’ Intent â†’ Cloud LLM + Real-time Data"""
    
    def __init__(self):
        """Initialize the complete cloud system"""
        print("ğŸŒ Initializing Complete Cloud Farmer Assistant...")
        print("=" * 70)
        
        # Initialize components
        self.initialize_components()
        
        # Session tracking
        self.session_start = datetime.now()
        self.total_interactions = 0
        self.successful_responses = 0
        
        # Audio processing state
        self.is_processing = False
        self.last_response_time = 0
    
    def initialize_components(self):
        """Initialize all system components"""
        
        # 1. Initialize STT
        print("ğŸ¤ Initializing Speech-to-Text...")
        try:
            self.stt = ImprovedSTT()
            print("âœ… STT system ready")
        except Exception as e:
            print(f"âŒ STT initialization failed: {e}")
            sys.exit(1)
        
        # 2. Initialize Cloud LLM (includes NLP)
        print("ğŸŒ Initializing Cloud LLM with Real-time Data...")
        try:
            self.cloud_llm = CloudLLMFarmerAssistant()
            print("âœ… Cloud LLM system ready")
        except Exception as e:
            print(f"âŒ Cloud LLM initialization failed: {e}")
            print("ğŸ’¡ Make sure you have API keys configured")
            sys.exit(1)
    
    def display_welcome(self):
        """Display welcome message"""
        print("\n" + "=" * 90)
        print("ğŸŒ à¤¸à¥à¤µà¤¾à¤—à¤¤ à¤¹à¥ˆ - Complete Cloud Farmer Assistant")
        print("ğŸ¤ â†’ ğŸ§  â†’ ğŸŒ â†’ ğŸ“Š (Speech â†’ Intent â†’ Cloud LLM â†’ Real-time Data)")
        print("=" * 90)
        print("ğŸ“‹ à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤•à¥€ à¤µà¤¿à¤¶à¥‡à¤·à¤¤à¤¾à¤à¤‚:")
        print("  â€¢ ğŸ¤ Real-time speech recognition (85-95% accuracy)")
        print("  â€¢ ğŸ§  94.4% accurate intent detection (30K samples)")
        print("  â€¢ ğŸŒ Cloud LLM responses (Groq/OpenAI/Gemini)")
        print("  â€¢ ğŸ“Š Real-time data integration (Weather/Market/News)")
        print("  â€¢ ğŸ—£ï¸ Hindi + English support")
        print("  â€¢ ğŸŒ¾ 48+ farming topics covered")
        print("=" * 90)
        
        # Show current LLM provider
        current_llm = self.cloud_llm.current_llm
        if current_llm:
            model = self.cloud_llm.llm_apis[current_llm]["model"]
            print(f"ğŸ¤– Current LLM: {current_llm} ({model})")
        
        # Show available data sources
        enabled_sources = [k for k, v in self.cloud_llm.data_sources.items() if v["enabled"]]
        if enabled_sources:
            print(f"ğŸ“Š Real-time Data: {', '.join(enabled_sources)}")
        else:
            print("ğŸ“Š Real-time Data: Not configured (optional)")
        
        print("=" * 90)
        print("ğŸ’¡ à¤¬à¥‹à¤²à¤•à¤° à¤…à¤ªà¤¨à¥‡ à¤–à¥‡à¤¤à¥€ à¤•à¥‡ à¤¸à¤µà¤¾à¤² à¤ªà¥‚à¤›à¥‡à¤‚")
        print("ğŸ›‘ à¤°à¥‹à¤•à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ Ctrl+C à¤¦à¤¬à¤¾à¤à¤‚")
        print("=" * 90)
    
    def process_complete_cloud_pipeline(self, transcribed_text: str):
        """Process through complete STT â†’ NLP â†’ Cloud LLM pipeline"""
        if not transcribed_text or len(transcribed_text.strip()) < 3:
            return
        
        if self.is_processing:
            return  # Avoid overlapping processing
        
        self.is_processing = True
        start_time = time.time()
        
        try:
            print(f"\nğŸ¤ à¤†à¤ªà¤¨à¥‡ à¤•à¤¹à¤¾: {transcribed_text}")
            print("ğŸ”„ Processing through cloud pipeline...")
            
            # Process through cloud LLM (includes NLP)
            result = self.cloud_llm.process_farmer_query(transcribed_text)
            
            # Extract results
            nlp_result = result["nlp_result"]
            llm_response = result["llm_response"]
            llm_provider = result["llm_provider"]
            
            intent = nlp_result["intent"]
            confidence = nlp_result["confidence"]
            entities = nlp_result["entities"]
            
            # Display Results
            processing_time = time.time() - start_time
            self.display_complete_cloud_response(
                transcribed_text, nlp_result, llm_response, 
                llm_provider, processing_time
            )
            
            # Update statistics
            self.total_interactions += 1
            if confidence > 0.5:  # Consider successful if confidence > 0.5
                self.successful_responses += 1
            
            self.last_response_time = processing_time
            
        except Exception as e:
            print(f"âŒ Cloud pipeline error: {e}")
            print("ğŸ’¡ Check your internet connection and API keys")
        finally:
            self.is_processing = False
    
    def display_complete_cloud_response(self, user_query: str, nlp_result: Dict, 
                                      llm_response: str, llm_provider: str, processing_time: float):
        """Display formatted complete cloud response"""
        print("\n" + "ğŸŒ" * 35)
        print("ğŸ¤– Cloud Farmer Assistant à¤•à¤¾ à¤œà¤µà¤¾à¤¬:")
        print("ğŸŒ" * 35)
        print(f"ğŸ’¬ {llm_response}")
        print("ğŸŒ" * 35)
        
        # Technical details
        print(f"â±ï¸ Total Response Time: {processing_time:.2f}s")
        print(f"ğŸŒ LLM Provider: {llm_provider}")
        print(f"ğŸ“Š NLP Confidence: {nlp_result['confidence']:.2f}")
        print(f"ğŸ¯ Intent: {nlp_result['intent']}")
        if nlp_result['entities']:
            print(f"ğŸ·ï¸ Entities: {nlp_result['entities']}")
        
        # Show real-time data usage
        enabled_sources = [k for k, v in self.cloud_llm.data_sources.items() if v["enabled"]]
        if enabled_sources:
            print(f"ğŸ“Š Real-time Data: {', '.join(enabled_sources)}")
        
        print("â”€" * 70)
        print("ğŸ¤ à¤…à¤—à¤²à¤¾ à¤¸à¤µà¤¾à¤² à¤¬à¥‹à¤²à¥‡à¤‚...")
    
    def run_complete_cloud_system(self):
        """Run the complete cloud integrated system"""
        try:
            # Display welcome
            self.display_welcome()
            
            # Setup STT
            if not self.stt.select_language():
                return
            
            if not self.stt.load_improved_model():
                return
            
            print("\nğŸ¯ Complete cloud system ready! Start speaking...")
            print("ğŸ’¡ Speak clearly and wait for intelligent response")
            print("-" * 80)
            
            # Start STT with Cloud LLM integration
            self.stt.is_running = True
            
            # Modified audio processing for cloud pipeline
            audio_thread = threading.Thread(target=self.cloud_audio_processing, daemon=True)
            audio_thread.start()
            
            # Start audio stream
            import sounddevice as sd
            with sd.RawInputStream(
                samplerate=self.stt.sample_rate,
                blocksize=self.stt.block_size,
                device=None,
                dtype='float32',
                channels=1,
                callback=self.stt.audio_callback
            ):
                print(f"ğŸ¤ Listening... (Sample rate: {self.stt.sample_rate} Hz)")
                
                while self.stt.is_running:
                    time.sleep(0.1)
                    
        except KeyboardInterrupt:
            print("\n\nğŸ›‘ Shutting down cloud system...")
            self.stt.is_running = False
            self.show_session_summary()
            
        except Exception as e:
            print(f"âŒ System error: {e}")
            self.stt.is_running = False
    
    def cloud_audio_processing(self):
        """Audio processing with cloud pipeline integration"""
        import queue
        import json
        import numpy as np
        
        last_partial = ""
        audio_count = 0
        
        while self.stt.is_running:
            try:
                # Get audio data
                data = self.stt.audio_queue.get(timeout=1)
                audio_count += 1
                
                # Show audio activity (less frequent for cloud processing)
                if audio_count % 20 == 0:  # Even less frequent for cloud
                    audio_array = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32767.0
                    volume = np.sqrt(np.mean(audio_array**2))
                    
                    bar_length = 12
                    filled_length = int(bar_length * min(volume * 20, 1.0))
                    bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                    
                    if volume > 0.05:
                        status = "ğŸŸ¢"
                    elif volume > 0.02:
                        status = "ğŸŸ¡"
                    else:
                        status = "ğŸ”´"
                    
                    print(f"\rğŸ¤ |{bar}| {status}", end='', flush=True)
                
                # Process with recognizer
                if self.stt.recognizer.AcceptWaveform(data):
                    # Final result - process through cloud pipeline
                    result = json.loads(self.stt.recognizer.Result())
                    text = result.get('text', '').strip()
                    
                    if text and len(text) > 2:
                        # Clear the audio bar
                        print("\r" + " " * 30 + "\r", end='')
                        
                        # Process through complete cloud pipeline
                        self.process_complete_cloud_pipeline(text)
                        
                        last_partial = ""
                else:
                    # Partial result (show less frequently)
                    partial = json.loads(self.stt.recognizer.PartialResult())
                    partial_text = partial.get('partial', '').strip()
                    
                    if partial_text and partial_text != last_partial and len(partial_text) > 5:
                        print(f"\rğŸ¤ à¤¸à¥à¤¨ à¤°à¤¹à¤¾ à¤¹à¥‚à¤‚: {partial_text[:30]}...", end='', flush=True)
                        last_partial = partial_text
                        
            except queue.Empty:
                continue
            except Exception as e:
                print(f"\nâŒ Audio processing error: {e}")
    
    def show_session_summary(self):
        """Show complete session statistics"""
        session_duration = datetime.now() - self.session_start
        
        print("\nğŸ“Š Complete Cloud Session Summary:")
        print("=" * 70)
        print(f"â±ï¸ Session Duration: {session_duration}")
        print(f"ğŸ’¬ Total Interactions: {self.total_interactions}")
        print(f"âœ… Successful Responses: {self.successful_responses}")
        
        if self.total_interactions > 0:
            success_rate = (self.successful_responses / self.total_interactions) * 100
            print(f"ğŸ“ˆ Success Rate: {success_rate:.1f}%")
        
        if self.last_response_time > 0:
            print(f"âš¡ Last Response Time: {self.last_response_time:.2f}s")
        
        # Show component summaries
        print("\nğŸ”§ Component Performance:")
        
        # Cloud LLM summary
        current_llm = self.cloud_llm.current_llm
        if current_llm:
            model = self.cloud_llm.llm_apis[current_llm]["model"]
            print(f"ğŸŒ LLM Provider: {current_llm} ({model})")
        
        # NLP summary
        nlp_summary = self.cloud_llm.nlp_detector.get_conversation_summary()
        if nlp_summary.get("total_interactions", 0) > 0:
            print(f"ğŸ§  NLP Accuracy: {nlp_summary['average_confidence']:.2f}")
            print(f"ğŸ¯ Top Intents: {list(nlp_summary['intent_distribution'].keys())[:3]}")
        
        # Data sources
        enabled_sources = [k for k, v in self.cloud_llm.data_sources.items() if v["enabled"]]
        if enabled_sources:
            print(f"ğŸ“Š Real-time Data: {', '.join(enabled_sources)}")
        
        print("\nğŸ™ à¤§à¤¨à¥à¤¯à¤µà¤¾à¤¦! Cloud-powered farming assistance à¤•à¥‡ à¤²à¤¿à¤!")


def main():
    """Main function"""
    try:
        assistant = CompleteCloudFarmerAssistant()
        assistant.run_complete_cloud_system()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ à¤…à¤²à¤µà¤¿à¤¦à¤¾!")
    except Exception as e:
        print(f"âŒ System startup failed: {e}")


if __name__ == "__main__":
    main()
