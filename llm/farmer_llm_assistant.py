#!/usr/bin/env python3
"""
Farmer LLM Assistant
Takes NLP intent output and generates human-like responses using LLM
"""

import json
import requests
import time
from datetime import datetime
from typing import Dict, List, Optional
import sys
import os

# Add parent directories to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'nlp'))

try:
    from csv_based_intent_detector import CSVBasedFarmerIntentDetector
except ImportError:
    print("‚ùå Could not import NLP module. Please ensure 'nlp' folder exists.")
    sys.exit(1)


class FarmerLLMAssistant:
    """Advanced LLM-powered farmer assistant"""
    
    def __init__(self):
        """Initialize the LLM assistant"""
        self.ollama_url = "http://localhost:11434"
        self.model_name = "llama3.2:3b"  # Good balance of speed and quality
        
        # Initialize NLP detector
        try:
            self.nlp_detector = CSVBasedFarmerIntentDetector()
            print("‚úÖ NLP system loaded successfully")
        except Exception as e:
            print(f"‚ùå Failed to load NLP system: {e}")
            sys.exit(1)
        
        # Session tracking
        self.conversation_history = []
        self.session_start = datetime.now()
        
        # Setup farmer-specific prompts
        self.setup_farmer_prompts()
        
        # Check Ollama availability
        self.check_ollama_availability()
    
    def setup_farmer_prompts(self):
        """Setup farmer-specific prompts for different intents"""
        self.farmer_prompts = {
            "seed_inquiry": {
                "system_prompt": """‡§Ü‡§™ ‡§è‡§ï ‡§Ö‡§®‡•Å‡§≠‡§µ‡•Ä ‡§ï‡•É‡§∑‡§ø ‡§µ‡§ø‡§∂‡•á‡§∑‡§ú‡•ç‡§û ‡§π‡•à‡§Ç ‡§ú‡•ã ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã‡§Ç ‡§ï‡•ã ‡§¨‡•Ä‡§ú ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§∏‡§≤‡§æ‡§π ‡§¶‡•á‡§§‡•á ‡§π‡•à‡§Ç‡•§ 
                ‡§Ü‡§™‡§ï‡•ã ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§∏‡§∞‡§≤ ‡§î‡§∞ ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§¶‡•á‡§®‡•Ä ‡§π‡•à‡•§ ‡§¨‡•Ä‡§ú ‡§ï‡•Ä ‡§ï‡§ø‡§∏‡•ç‡§Æ, ‡§¨‡•Å‡§Ü‡§à ‡§ï‡§æ ‡§∏‡§Æ‡§Ø, 
                ‡§Æ‡§æ‡§§‡•ç‡§∞‡§æ, ‡§î‡§∞ ‡§ï‡§π‡§æ‡§Å ‡§∏‡•á ‡§ñ‡§∞‡•Ä‡§¶‡§®‡§æ ‡§π‡•à - ‡§á‡§® ‡§∏‡§≠‡•Ä ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§¶‡•á‡§Ç‡•§""",
                
                "context": "‡§¨‡•Ä‡§ú ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§î‡§∞ ‡§∏‡§≤‡§æ‡§π"
            },
            
            "fertilizer_advice": {
                "system_prompt": """‡§Ü‡§™ ‡§è‡§ï ‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä ‡§î‡§∞ ‡§â‡§∞‡•ç‡§µ‡§∞‡§ï ‡§µ‡§ø‡§∂‡•á‡§∑‡§ú‡•ç‡§û ‡§π‡•à‡§Ç‡•§ ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã‡§Ç ‡§ï‡•ã ‡§ñ‡§æ‡§¶ ‡§î‡§∞ ‡§â‡§∞‡•ç‡§µ‡§∞‡§ï ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç 
                ‡§∏‡§∞‡§≤ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§∏‡§≤‡§æ‡§π ‡§¶‡•á‡§Ç‡•§ NPK ‡§Ö‡§®‡•Å‡§™‡§æ‡§§, ‡§Æ‡§æ‡§§‡•ç‡§∞‡§æ, ‡§∏‡§Æ‡§Ø, ‡§î‡§∞ ‡§ï‡•Ä‡§Æ‡§§ ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§¶‡•á‡§Ç‡•§ 
                ‡§ú‡•à‡§µ‡§ø‡§ï ‡§î‡§∞ ‡§∞‡§æ‡§∏‡§æ‡§Ø‡§®‡§ø‡§ï ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™ ‡§¨‡§§‡§æ‡§è‡§Ç‡•§""",
                
                "context": "‡§ñ‡§æ‡§¶ ‡§î‡§∞ ‡§â‡§∞‡•ç‡§µ‡§∞‡§ï ‡§ï‡•Ä ‡§∏‡§≤‡§æ‡§π"
            },
            
            "crop_disease": {
                "system_prompt": """‡§Ü‡§™ ‡§è‡§ï ‡§™‡•å‡§ß‡•ã‡§Ç ‡§ï‡•á ‡§∞‡•ã‡§ó ‡§µ‡§ø‡§∂‡•á‡§∑‡§ú‡•ç‡§û ‡§π‡•à‡§Ç‡•§ ‡§´‡§∏‡§≤ ‡§ï‡•Ä ‡§¨‡•Ä‡§Æ‡§æ‡§∞‡•Ä ‡§î‡§∞ ‡§ï‡•Ä‡§ü ‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡§æ 
                ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§® ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§Ç‡•§ ‡§≤‡§ï‡•ç‡§∑‡§£ ‡§™‡§π‡§ö‡§æ‡§®‡§®‡§æ, ‡§á‡§≤‡§æ‡§ú, ‡§¶‡§µ‡§æ‡§à, ‡§î‡§∞ ‡§¨‡§ö‡§æ‡§µ ‡§ï‡•á ‡§§‡§∞‡•Ä‡§ï‡•á ‡§¨‡§§‡§æ‡§è‡§Ç‡•§ 
                ‡§§‡•Å‡§∞‡§Ç‡§§ ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§â‡§™‡§æ‡§Ø ‡§™‡§∞ ‡§ú‡•ã‡§∞ ‡§¶‡•á‡§Ç‡•§""",
                
                "context": "‡§´‡§∏‡§≤ ‡§∞‡•ã‡§ó ‡§î‡§∞ ‡§ï‡•Ä‡§ü ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§£"
            },
            
            "market_price": {
                "system_prompt": """‡§Ü‡§™ ‡§è‡§ï ‡§ï‡•É‡§∑‡§ø ‡§Æ‡§æ‡§∞‡•ç‡§ï‡•á‡§ü‡§ø‡§Ç‡§ó ‡§µ‡§ø‡§∂‡•á‡§∑‡§ú‡•ç‡§û ‡§π‡•à‡§Ç‡•§ ‡§Æ‡§Ç‡§°‡•Ä ‡§≠‡§æ‡§µ, ‡§¨‡§æ‡§ú‡§æ‡§∞ ‡§ï‡•Ä ‡§∏‡•ç‡§•‡§ø‡§§‡§ø, 
                ‡§î‡§∞ ‡§¨‡•á‡§ö‡§®‡•á ‡§ï‡•Ä ‡§∏‡§≤‡§æ‡§π ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§Ç‡•§ ‡§ï‡•Ä‡§Æ‡§§ ‡§ï‡•á ‡§∞‡•Å‡§ù‡§æ‡§®, ‡§¨‡•á‡§π‡§§‡§∞ ‡§Æ‡§Ç‡§°‡•Ä, ‡§î‡§∞ ‡§¨‡§ø‡§ï‡•ç‡§∞‡•Ä ‡§ï‡§æ ‡§∏‡§Æ‡§Ø ‡§¨‡§§‡§æ‡§è‡§Ç‡•§ 
                eNAM ‡§î‡§∞ ‡§Ö‡§®‡•ç‡§Ø ‡§™‡•ç‡§≤‡•á‡§ü‡§´‡•â‡§∞‡•ç‡§Æ ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§¶‡•á‡§Ç‡•§""",
                
                "context": "‡§Æ‡§Ç‡§°‡•Ä ‡§≠‡§æ‡§µ ‡§î‡§∞ ‡§¨‡§æ‡§ú‡§æ‡§∞ ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä"
            },
            
            "general": {
                "system_prompt": """‡§Ü‡§™ ‡§è‡§ï ‡§Ö‡§®‡•Å‡§≠‡§µ‡•Ä ‡§ï‡§ø‡§∏‡§æ‡§® ‡§î‡§∞ ‡§ï‡•É‡§∑‡§ø ‡§∏‡§≤‡§æ‡§π‡§ï‡§æ‡§∞ ‡§π‡•à‡§Ç‡•§ ‡§ï‡§ø‡§∏‡§æ‡§®‡•ã‡§Ç ‡§ï‡•Ä ‡§π‡§∞ ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡§æ 
                ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§® ‡§∏‡§∞‡§≤ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§Ç‡•§ ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï, ‡§§‡•Å‡§∞‡§Ç‡§§ ‡§≤‡§æ‡§ó‡•Ç ‡§π‡•ã‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§∏‡§≤‡§æ‡§π ‡§¶‡•á‡§Ç‡•§ 
                ‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø ‡§™‡§∞‡§ø‡§∏‡•ç‡§•‡§ø‡§§‡§ø‡§Ø‡•ã‡§Ç ‡§ï‡•ã ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§Æ‡•á‡§Ç ‡§∞‡§ñ‡•á‡§Ç‡•§""",
                
                "context": "‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ï‡•É‡§∑‡§ø ‡§∏‡§≤‡§æ‡§π"
            }
        }
    
    def check_ollama_availability(self):
        """Check if Ollama is running and model is available"""
        try:
            # Check if Ollama is running
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [model["name"] for model in models]
                
                if self.model_name in model_names:
                    print(f"‚úÖ Ollama is running with {self.model_name}")
                    return True
                else:
                    print(f"‚ö†Ô∏è Model {self.model_name} not found. Available models: {model_names}")
                    print(f"üîÑ Attempting to pull {self.model_name}...")
                    self.pull_model()
                    return True
            else:
                print("‚ùå Ollama is not responding")
                return False
                
        except requests.exceptions.RequestException:
            print("‚ùå Ollama is not running. Please start Ollama first.")
            print("üí° Install Ollama from: https://ollama.ai")
            print("üí° Then run: ollama serve")
            return False
    
    def pull_model(self):
        """Pull the required model if not available"""
        try:
            print(f"üì• Pulling {self.model_name}... This may take a few minutes.")
            
            pull_data = {"name": self.model_name}
            response = requests.post(
                f"{self.ollama_url}/api/pull",
                json=pull_data,
                timeout=300  # 5 minutes timeout
            )
            
            if response.status_code == 200:
                print(f"‚úÖ Successfully pulled {self.model_name}")
            else:
                print(f"‚ùå Failed to pull {self.model_name}")
                
        except Exception as e:
            print(f"‚ùå Error pulling model: {e}")
    
    def generate_llm_response(self, intent_result: Dict, user_query: str) -> str:
        """Generate human-like response using LLM"""
        intent = intent_result.get("intent", "general")
        confidence = intent_result.get("confidence", 0.0)
        entities = intent_result.get("entities", {})
        
        # Get appropriate prompt
        prompt_config = self.farmer_prompts.get(intent, self.farmer_prompts["general"])
        system_prompt = prompt_config["system_prompt"]
        context = prompt_config["context"]
        
        # Build user prompt with context
        user_prompt = f"""
‡§ï‡§ø‡§∏‡§æ‡§® ‡§ï‡§æ ‡§∏‡§µ‡§æ‡§≤: "{user_query}"

‡§™‡§π‡§ö‡§æ‡§®‡§æ ‡§ó‡§Ø‡§æ ‡§µ‡§ø‡§∑‡§Ø: {context}
‡§µ‡§ø‡§∂‡•ç‡§µ‡§∏‡§®‡•Ä‡§Ø‡§§‡§æ: {confidence:.2f}

"""
        
        # Add entity information if available
        if entities:
            user_prompt += "‡§™‡§π‡§ö‡§æ‡§®‡•Ä ‡§ó‡§à ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä:\n"
            if "crops" in entities:
                user_prompt += f"- ‡§´‡§∏‡§≤: {', '.join(entities['crops'])}\n"
            if "quantities" in entities:
                user_prompt += f"- ‡§Æ‡§æ‡§§‡•ç‡§∞‡§æ: {', '.join(entities['quantities'])}\n"
            if "time" in entities:
                user_prompt += f"- ‡§∏‡§Æ‡§Ø: {', '.join(entities['time'])}\n"
            user_prompt += "\n"
        
        user_prompt += """
‡§ï‡•É‡§™‡§Ø‡§æ ‡§á‡§∏ ‡§ï‡§ø‡§∏‡§æ‡§® ‡§ï‡•ã ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§î‡§∞ ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä ‡§∏‡§≤‡§æ‡§π ‡§¶‡•á‡§Ç‡•§ ‡§ú‡§µ‡§æ‡§¨ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç, ‡§∏‡§∞‡§≤ ‡§≠‡§æ‡§∑‡§æ ‡§Æ‡•á‡§Ç, ‡§î‡§∞ ‡§§‡•Å‡§∞‡§Ç‡§§ ‡§≤‡§æ‡§ó‡•Ç ‡§π‡•ã‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§π‡•ã‡•§
‡§ú‡§µ‡§æ‡§¨ 3-4 ‡§µ‡§æ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§Ç, ‡§¨‡§π‡•Å‡§§ ‡§≤‡§Ç‡§¨‡§æ ‡§® ‡§ï‡§∞‡•á‡§Ç‡•§
"""
        
        try:
            # Prepare request for Ollama
            ollama_request = {
                "model": self.model_name,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "stream": False,
                "options": {
                    "temperature": 0.7,  # Balanced creativity
                    "top_p": 0.9,
                    "max_tokens": 200,   # Keep responses concise
                    "stop": ["\n\n", "‡§ï‡§ø‡§∏‡§æ‡§®:", "‡§∏‡§µ‡§æ‡§≤:"]
                }
            }
            
            # Make request to Ollama
            start_time = time.time()
            response = requests.post(
                f"{self.ollama_url}/api/chat",
                json=ollama_request,
                timeout=30
            )
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                llm_response = result["message"]["content"].strip()
                
                # Clean up response
                llm_response = self.clean_llm_response(llm_response)
                
                print(f"ü§ñ LLM Response Time: {response_time:.2f}s")
                return llm_response
            else:
                print(f"‚ùå LLM API Error: {response.status_code}")
                return self.get_fallback_response(intent, entities)
                
        except Exception as e:
            print(f"‚ùå LLM Error: {e}")
            return self.get_fallback_response(intent, entities)
    
    def clean_llm_response(self, response: str) -> str:
        """Clean and format LLM response"""
        # Remove unwanted prefixes/suffixes
        unwanted_prefixes = [
            "‡§ï‡§ø‡§∏‡§æ‡§® ‡§ú‡•Ä,", "‡§≠‡§æ‡§à ‡§∏‡§æ‡§π‡§¨,", "‡§ú‡•Ä ‡§π‡§æ‡§Å,", "‡§¶‡•á‡§ñ‡§ø‡§è,", 
            "‡§Ü‡§™‡§ï‡•ã ‡§¨‡§§‡§æ‡§®‡§æ ‡§ö‡§æ‡§π‡•Ç‡§Ç‡§ó‡§æ ‡§ï‡§ø", "‡§Æ‡•á‡§∞‡•Ä ‡§∏‡§≤‡§æ‡§π ‡§π‡•à ‡§ï‡§ø"
        ]
        
        for prefix in unwanted_prefixes:
            if response.startswith(prefix):
                response = response[len(prefix):].strip()
        
        # Ensure proper Hindi formatting
        response = response.replace("‡•§‡•§", "‡•§")
        response = response.strip()
        
        # Add appropriate ending if missing
        if not response.endswith(('‡•§', '‡•§', '!')):
            response += "‡•§"
        
        return response
    
    def get_fallback_response(self, intent: str, entities: Dict) -> str:
        """Provide fallback response when LLM fails"""
        fallback_responses = {
            "seed_inquiry": "‡§¨‡•Ä‡§ú ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ï‡•á ‡§≤‡§ø‡§è ‡§®‡§ú‡§¶‡•Ä‡§ï‡•Ä ‡§ï‡•É‡§∑‡§ø ‡§ï‡•á‡§Ç‡§¶‡•ç‡§∞ ‡§Ø‡§æ ‡§¨‡•Ä‡§ú ‡§≠‡§Ç‡§°‡§æ‡§∞ ‡§∏‡•á ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ï‡§∞‡•á‡§Ç‡•§ ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§ï‡§ø‡§∏‡•ç‡§Æ ‡§ï‡•á ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£‡§ø‡§§ ‡§¨‡•Ä‡§ú ‡§π‡•Ä ‡§ñ‡§∞‡•Ä‡§¶‡•á‡§Ç‡•§",
            
            "fertilizer_advice": "‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§ï‡§∞‡§æ‡§ï‡§∞ ‡§â‡§∏‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ ‡§∏‡§Ç‡§§‡•Å‡§≤‡§ø‡§§ ‡§â‡§∞‡•ç‡§µ‡§∞‡§ï ‡§ï‡§æ ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§ NPK ‡§Ö‡§®‡•Å‡§™‡§æ‡§§ ‡§ï‡§æ ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§∞‡§ñ‡•á‡§Ç‡•§",
            
            "crop_disease": "‡§´‡§∏‡§≤ ‡§Æ‡•á‡§Ç ‡§∞‡•ã‡§ó ‡§ï‡•á ‡§≤‡§ï‡•ç‡§∑‡§£ ‡§¶‡§ø‡§ñ‡§®‡•á ‡§™‡§∞ ‡§§‡•Å‡§∞‡§Ç‡§§ ‡§ï‡•É‡§∑‡§ø ‡§µ‡§ø‡§∂‡•á‡§∑‡§ú‡•ç‡§û ‡§∏‡•á ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ï‡§∞‡•á‡§Ç‡•§ ‡§∏‡§π‡•Ä ‡§¶‡§µ‡§æ‡§à ‡§ï‡§æ ‡§õ‡§ø‡§°‡§º‡§ï‡§æ‡§µ ‡§ï‡§∞‡•á‡§Ç‡•§",
            
            "market_price": "‡§Æ‡§Ç‡§°‡•Ä ‡§≠‡§æ‡§µ ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ï‡•á ‡§≤‡§ø‡§è eNAM ‡§™‡•ã‡§∞‡•ç‡§ü‡§≤ ‡§¶‡•á‡§ñ‡•á‡§Ç ‡§Ø‡§æ ‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø ‡§Æ‡§Ç‡§°‡•Ä ‡§∏‡•á ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ï‡§∞‡•á‡§Ç‡•§",
            
            "general": "‡§Ü‡§™‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§®‡§ú‡§¶‡•Ä‡§ï‡•Ä ‡§ï‡•É‡§∑‡§ø ‡§µ‡§ø‡§ú‡•ç‡§û‡§æ‡§® ‡§ï‡•á‡§Ç‡§¶‡•ç‡§∞ ‡§Ø‡§æ ‡§ï‡•É‡§∑‡§ø ‡§µ‡§ø‡§≠‡§æ‡§ó ‡§∏‡•á ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ï‡§∞‡•á‡§Ç‡•§"
        }
        
        base_response = fallback_responses.get(intent, fallback_responses["general"])
        
        # Add entity-specific information
        if "crops" in entities:
            crops = ", ".join(entities["crops"])
            base_response = f"{crops} ‡§ï‡•á ‡§≤‡§ø‡§è {base_response}"
        
        return base_response
    
    def process_farmer_query(self, user_query: str) -> Dict:
        """Complete pipeline: NLP ‚Üí LLM ‚Üí Response"""
        print(f"\nüåæ Processing: {user_query}")
        
        # Step 1: NLP Intent Detection
        print("üîç Step 1: Detecting intent...")
        intent_result = self.nlp_detector.detect_intent(user_query)
        
        intent = intent_result["intent"]
        confidence = intent_result["confidence"]
        entities = intent_result["entities"]
        
        print(f"üéØ Intent: {intent} (Confidence: {confidence:.2f})")
        if entities:
            print(f"üè∑Ô∏è Entities: {entities}")
        
        # Step 2: LLM Response Generation
        print("ü§ñ Step 2: Generating LLM response...")
        llm_response = self.generate_llm_response(intent_result, user_query)
        
        # Step 3: Build complete result
        result = {
            "user_query": user_query,
            "nlp_result": intent_result,
            "llm_response": llm_response,
            "timestamp": datetime.now().isoformat(),
            "processing_pipeline": "STT ‚Üí NLP ‚Üí LLM"
        }
        
        # Add to conversation history
        self.conversation_history.append(result)
        
        return result
    
    def display_response(self, result: Dict):
        """Display formatted response to user"""
        print("\n" + "="*60)
        print("üåæ ‡§ï‡§ø‡§∏‡§æ‡§® ‡§∏‡§π‡§æ‡§Ø‡§ï ‡§ï‡§æ ‡§ú‡§µ‡§æ‡§¨:")
        print("="*60)
        print(f"üí¨ {result['llm_response']}")
        print("="*60)
        
        # Show technical details
        nlp_result = result["nlp_result"]
        print(f"üìä ‡§µ‡§ø‡§∂‡•ç‡§µ‡§∏‡§®‡•Ä‡§Ø‡§§‡§æ: {nlp_result['confidence']:.2f}")
        print(f"üéØ ‡§µ‡§ø‡§∑‡§Ø: {nlp_result['intent']}")
        if nlp_result['entities']:
            print(f"üè∑Ô∏è ‡§™‡§π‡§ö‡§æ‡§®‡•Ä ‡§ó‡§à ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä: {nlp_result['entities']}")
    
    def get_conversation_summary(self) -> Dict:
        """Get conversation summary"""
        if not self.conversation_history:
            return {"message": "No conversation history"}
        
        total_queries = len(self.conversation_history)
        intents = [entry["nlp_result"]["intent"] for entry in self.conversation_history]
        intent_counts = {}
        
        for intent in intents:
            intent_counts[intent] = intent_counts.get(intent, 0) + 1
        
        avg_confidence = sum(entry["nlp_result"]["confidence"] for entry in self.conversation_history) / total_queries
        
        session_duration = datetime.now() - self.session_start
        
        return {
            "session_duration": str(session_duration),
            "total_queries": total_queries,
            "intent_distribution": intent_counts,
            "average_confidence": avg_confidence,
            "llm_model": self.model_name
        }


def main():
    """Interactive farmer assistant"""
    print("üåæ Farmer LLM Assistant - STT ‚Üí NLP ‚Üí LLM Pipeline")
    print("="*70)
    
    try:
        # Initialize assistant
        assistant = FarmerLLMAssistant()
        
        print("\n‚úÖ System Ready!")
        print("üí° Type your farming questions in Hindi or English")
        print("üí° Type 'quit' to exit")
        print("üí° Type 'summary' to see conversation summary")
        print("-"*70)
        
        while True:
            try:
                # Get user input
                user_input = input("\nüé§ ‡§Ü‡§™‡§ï‡§æ ‡§∏‡§µ‡§æ‡§≤: ").strip()
                
                if user_input.lower() in ['quit', 'exit', '‡§¨‡§æ‡§π‡§∞', '‡§¨‡§Ç‡§¶']:
                    print("\nüëã ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶! ‡§ñ‡•á‡§§‡•Ä ‡§Æ‡•á‡§Ç ‡§∏‡§´‡§≤‡§§‡§æ ‡§ï‡•Ä ‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ‡§è‡§Ç!")
                    break
                
                if user_input.lower() in ['summary', '‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂']:
                    summary = assistant.get_conversation_summary()
                    print("\nüìä Conversation Summary:")
                    print(json.dumps(summary, indent=2, ensure_ascii=False))
                    continue
                
                if not user_input:
                    print("‚ö†Ô∏è ‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö‡§™‡§®‡§æ ‡§∏‡§µ‡§æ‡§≤ ‡§≤‡§ø‡§ñ‡•á‡§Ç‡•§")
                    continue
                
                # Process query through complete pipeline
                result = assistant.process_farmer_query(user_input)
                
                # Display response
                assistant.display_response(result)
                
            except KeyboardInterrupt:
                print("\n\nüëã ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§¨‡§Ç‡§¶ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç...")
                break
            except Exception as e:
                print(f"\n‚ùå Error: {e}")
                continue
        
        # Show final summary
        summary = assistant.get_conversation_summary()
        if summary.get("total_queries", 0) > 0:
            print("\nüìà Final Session Summary:")
            print(json.dumps(summary, indent=2, ensure_ascii=False))
            
    except Exception as e:
        print(f"‚ùå System initialization failed: {e}")


if __name__ == "__main__":
    main()
