#!/usr/bin/env python3
"""
Farmer LLM Assistant
Takes NLP intent output and generates human-like responses using LLM
"""

import json
import requests
import time
from datetime import datetime
from typing import Dict, List, Optional
import sys
import os

# Add parent directories to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'nlp'))

try:
    from csv_based_intent_detector import CSVBasedFarmerIntentDetector
except ImportError:
    print("âŒ Could not import NLP module. Please ensure 'nlp' folder exists.")
    sys.exit(1)


class FarmerLLMAssistant:
    """Advanced LLM-powered farmer assistant"""
    
    def __init__(self):
        """Initialize the LLM assistant"""
        self.ollama_url = "http://localhost:11434"
        self.model_name = "llama3.2:3b"  # Good balance of speed and quality
        
        # Initialize NLP detector
        try:
            self.nlp_detector = CSVBasedFarmerIntentDetector()
            print("âœ… NLP system loaded successfully")
        except Exception as e:
            print(f"âŒ Failed to load NLP system: {e}")
            sys.exit(1)
        
        # Session tracking
        self.conversation_history = []
        self.session_start = datetime.now()
        
        # Setup farmer-specific prompts
        self.setup_farmer_prompts()
        
        # Check Ollama availability
        self.check_ollama_availability()
    
    def setup_farmer_prompts(self):
        """Setup farmer-specific prompts for different intents"""
        self.farmer_prompts = {
            "seed_inquiry": {
                "system_prompt": """à¤†à¤ª à¤à¤• à¤…à¤¨à¥à¤­à¤µà¥€ à¤•à¥ƒà¤·à¤¿ à¤µà¤¿à¤¶à¥‡à¤·à¤œà¥à¤ à¤¹à¥ˆà¤‚ à¤œà¥‹ à¤•à¤¿à¤¸à¤¾à¤¨à¥‹à¤‚ à¤•à¥‹ à¤¬à¥€à¤œ à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤¸à¤²à¤¾à¤¹ à¤¦à¥‡à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ 
                à¤†à¤ªà¤•à¥‹ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤¸à¤°à¤² à¤”à¤° à¤µà¥à¤¯à¤¾à¤µà¤¹à¤¾à¤°à¤¿à¤• à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤¦à¥‡à¤¨à¥€ à¤¹à¥ˆà¥¤ à¤¬à¥€à¤œ à¤•à¥€ à¤•à¤¿à¤¸à¥à¤®, à¤¬à¥à¤†à¤ˆ à¤•à¤¾ à¤¸à¤®à¤¯, 
                à¤®à¤¾à¤¤à¥à¤°à¤¾, à¤”à¤° à¤•à¤¹à¤¾à¤ à¤¸à¥‡ à¤–à¤°à¥€à¤¦à¤¨à¤¾ à¤¹à¥ˆ - à¤‡à¤¨ à¤¸à¤­à¥€ à¤•à¥€ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤¦à¥‡à¤‚à¥¤""",
                
                "context": "à¤¬à¥€à¤œ à¤•à¥€ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤”à¤° à¤¸à¤²à¤¾à¤¹"
            },
            
            "fertilizer_advice": {
                "system_prompt": """à¤†à¤ª à¤à¤• à¤®à¤¿à¤Ÿà¥à¤Ÿà¥€ à¤”à¤° à¤‰à¤°à¥à¤µà¤°à¤• à¤µà¤¿à¤¶à¥‡à¤·à¤œà¥à¤ à¤¹à¥ˆà¤‚à¥¤ à¤•à¤¿à¤¸à¤¾à¤¨à¥‹à¤‚ à¤•à¥‹ à¤–à¤¾à¤¦ à¤”à¤° à¤‰à¤°à¥à¤µà¤°à¤• à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ 
                à¤¸à¤°à¤² à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤¸à¤²à¤¾à¤¹ à¤¦à¥‡à¤‚à¥¤ NPK à¤…à¤¨à¥à¤ªà¤¾à¤¤, à¤®à¤¾à¤¤à¥à¤°à¤¾, à¤¸à¤®à¤¯, à¤”à¤° à¤•à¥€à¤®à¤¤ à¤•à¥€ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤¦à¥‡à¤‚à¥¤ 
                à¤œà¥ˆà¤µà¤¿à¤• à¤”à¤° à¤°à¤¾à¤¸à¤¾à¤¯à¤¨à¤¿à¤• à¤¦à¥‹à¤¨à¥‹à¤‚ à¤µà¤¿à¤•à¤²à¥à¤ª à¤¬à¤¤à¤¾à¤à¤‚à¥¤""",
                
                "context": "à¤–à¤¾à¤¦ à¤”à¤° à¤‰à¤°à¥à¤µà¤°à¤• à¤•à¥€ à¤¸à¤²à¤¾à¤¹"
            },
            
            "crop_disease": {
                "system_prompt": """à¤†à¤ª à¤à¤• à¤ªà¥Œà¤§à¥‹à¤‚ à¤•à¥‡ à¤°à¥‹à¤— à¤µà¤¿à¤¶à¥‡à¤·à¤œà¥à¤ à¤¹à¥ˆà¤‚à¥¤ à¤«à¤¸à¤² à¤•à¥€ à¤¬à¥€à¤®à¤¾à¤°à¥€ à¤”à¤° à¤•à¥€à¤Ÿ à¤•à¥€ à¤¸à¤®à¤¸à¥à¤¯à¤¾ à¤•à¤¾ 
                à¤¸à¤®à¤¾à¤§à¤¾à¤¨ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤¦à¥‡à¤‚à¥¤ à¤²à¤•à¥à¤·à¤£ à¤ªà¤¹à¤šà¤¾à¤¨à¤¨à¤¾, à¤‡à¤²à¤¾à¤œ, à¤¦à¤µà¤¾à¤ˆ, à¤”à¤° à¤¬à¤šà¤¾à¤µ à¤•à¥‡ à¤¤à¤°à¥€à¤•à¥‡ à¤¬à¤¤à¤¾à¤à¤‚à¥¤ 
                à¤¤à¥à¤°à¤‚à¤¤ à¤•à¤°à¤¨à¥‡ à¤µà¤¾à¤²à¥‡ à¤‰à¤ªà¤¾à¤¯ à¤ªà¤° à¤œà¥‹à¤° à¤¦à¥‡à¤‚à¥¤""",
                
                "context": "à¤«à¤¸à¤² à¤°à¥‹à¤— à¤”à¤° à¤•à¥€à¤Ÿ à¤¨à¤¿à¤¯à¤‚à¤¤à¥à¤°à¤£"
            },
            
            "market_price": {
                "system_prompt": """à¤†à¤ª à¤à¤• à¤•à¥ƒà¤·à¤¿ à¤®à¤¾à¤°à¥à¤•à¥‡à¤Ÿà¤¿à¤‚à¤— à¤µà¤¿à¤¶à¥‡à¤·à¤œà¥à¤ à¤¹à¥ˆà¤‚à¥¤ à¤®à¤‚à¤¡à¥€ à¤­à¤¾à¤µ, à¤¬à¤¾à¤œà¤¾à¤° à¤•à¥€ à¤¸à¥à¤¥à¤¿à¤¤à¤¿, 
                à¤”à¤° à¤¬à¥‡à¤šà¤¨à¥‡ à¤•à¥€ à¤¸à¤²à¤¾à¤¹ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤¦à¥‡à¤‚à¥¤ à¤•à¥€à¤®à¤¤ à¤•à¥‡ à¤°à¥à¤à¤¾à¤¨, à¤¬à¥‡à¤¹à¤¤à¤° à¤®à¤‚à¤¡à¥€, à¤”à¤° à¤¬à¤¿à¤•à¥à¤°à¥€ à¤•à¤¾ à¤¸à¤®à¤¯ à¤¬à¤¤à¤¾à¤à¤‚à¥¤ 
                eNAM à¤”à¤° à¤…à¤¨à¥à¤¯ à¤ªà¥à¤²à¥‡à¤Ÿà¤«à¥‰à¤°à¥à¤® à¤•à¥€ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤¦à¥‡à¤‚à¥¤""",
                
                "context": "à¤®à¤‚à¤¡à¥€ à¤­à¤¾à¤µ à¤”à¤° à¤¬à¤¾à¤œà¤¾à¤° à¤•à¥€ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€"
            },
            
            "general": {
                "system_prompt": """à¤†à¤ª à¤à¤• à¤…à¤¨à¥à¤­à¤µà¥€ à¤•à¤¿à¤¸à¤¾à¤¨ à¤”à¤° à¤•à¥ƒà¤·à¤¿ à¤¸à¤²à¤¾à¤¹à¤•à¤¾à¤° à¤¹à¥ˆà¤‚à¥¤ à¤•à¤¿à¤¸à¤¾à¤¨à¥‹à¤‚ à¤•à¥€ à¤¹à¤° à¤¸à¤®à¤¸à¥à¤¯à¤¾ à¤•à¤¾ 
                à¤¸à¤®à¤¾à¤§à¤¾à¤¨ à¤¸à¤°à¤² à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤¦à¥‡à¤‚à¥¤ à¤µà¥à¤¯à¤¾à¤µà¤¹à¤¾à¤°à¤¿à¤•, à¤¤à¥à¤°à¤‚à¤¤ à¤²à¤¾à¤—à¥‚ à¤¹à¥‹à¤¨à¥‡ à¤µà¤¾à¤²à¥€ à¤¸à¤²à¤¾à¤¹ à¤¦à¥‡à¤‚à¥¤ 
                à¤¸à¥à¤¥à¤¾à¤¨à¥€à¤¯ à¤ªà¤°à¤¿à¤¸à¥à¤¥à¤¿à¤¤à¤¿à¤¯à¥‹à¤‚ à¤•à¥‹ à¤§à¥à¤¯à¤¾à¤¨ à¤®à¥‡à¤‚ à¤°à¤–à¥‡à¤‚à¥¤""",
                
                "context": "à¤¸à¤¾à¤®à¤¾à¤¨à¥à¤¯ à¤•à¥ƒà¤·à¤¿ à¤¸à¤²à¤¾à¤¹"
            }
        }
    
    def check_ollama_availability(self):
        """Check if Ollama is running and model is available"""
        try:
            # Check if Ollama is running
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [model["name"] for model in models]
                
                if self.model_name in model_names:
                    print(f"âœ… Ollama is running with {self.model_name}")
                    return True
                else:
                    print(f"âš ï¸ Model {self.model_name} not found. Available models: {model_names}")
                    print(f"ğŸ”„ Attempting to pull {self.model_name}...")
                    self.pull_model()
                    return True
            else:
                print("âŒ Ollama is not responding")
                return False
                
        except requests.exceptions.RequestException:
            print("âŒ Ollama is not running. Please start Ollama first.")
            print("ğŸ’¡ Install Ollama from: https://ollama.ai")
            print("ğŸ’¡ Then run: ollama serve")
            return False
    
    def pull_model(self):
        """Pull the required model if not available"""
        try:
            print(f"ğŸ“¥ Pulling {self.model_name}... This may take a few minutes.")
            
            pull_data = {"name": self.model_name}
            response = requests.post(
                f"{self.ollama_url}/api/pull",
                json=pull_data,
                timeout=300  # 5 minutes timeout
            )
            
            if response.status_code == 200:
                print(f"âœ… Successfully pulled {self.model_name}")
            else:
                print(f"âŒ Failed to pull {self.model_name}")
                
        except Exception as e:
            print(f"âŒ Error pulling model: {e}")
    
    def generate_llm_response(self, intent_result: Dict, user_query: str) -> str:
        """Generate human-like response using LLM"""
        intent = intent_result.get("intent", "general")
        confidence = intent_result.get("confidence", 0.0)
        entities = intent_result.get("entities", {})
        
        # Get appropriate prompt
        prompt_config = self.farmer_prompts.get(intent, self.farmer_prompts["general"])
        system_prompt = prompt_config["system_prompt"]
        context = prompt_config["context"]
        
        # Build user prompt with context
        user_prompt = f"""
à¤•à¤¿à¤¸à¤¾à¤¨ à¤•à¤¾ à¤¸à¤µà¤¾à¤²: "{user_query}"

à¤ªà¤¹à¤šà¤¾à¤¨à¤¾ à¤—à¤¯à¤¾ à¤µà¤¿à¤·à¤¯: {context}
à¤µà¤¿à¤¶à¥à¤µà¤¸à¤¨à¥€à¤¯à¤¤à¤¾: {confidence:.2f}

"""
        
        # Add entity information if available
        if entities:
            user_prompt += "à¤ªà¤¹à¤šà¤¾à¤¨à¥€ à¤—à¤ˆ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€:\n"
            if "crops" in entities:
                user_prompt += f"- à¤«à¤¸à¤²: {', '.join(entities['crops'])}\n"
            if "quantities" in entities:
                user_prompt += f"- à¤®à¤¾à¤¤à¥à¤°à¤¾: {', '.join(entities['quantities'])}\n"
            if "time" in entities:
                user_prompt += f"- à¤¸à¤®à¤¯: {', '.join(entities['time'])}\n"
            user_prompt += "\n"
        
        user_prompt += """
à¤•à¥ƒà¤ªà¤¯à¤¾ à¤‡à¤¸ à¤•à¤¿à¤¸à¤¾à¤¨ à¤•à¥‹ à¤µà¥à¤¯à¤¾à¤µà¤¹à¤¾à¤°à¤¿à¤• à¤”à¤° à¤‰à¤ªà¤¯à¥‹à¤—à¥€ à¤¸à¤²à¤¾à¤¹ à¤¦à¥‡à¤‚à¥¤ à¤œà¤µà¤¾à¤¬ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚, à¤¸à¤°à¤² à¤­à¤¾à¤·à¤¾ à¤®à¥‡à¤‚, à¤”à¤° à¤¤à¥à¤°à¤‚à¤¤ à¤²à¤¾à¤—à¥‚ à¤¹à¥‹à¤¨à¥‡ à¤µà¤¾à¤²à¤¾ à¤¹à¥‹à¥¤
à¤œà¤µà¤¾à¤¬ 3-4 à¤µà¤¾à¤•à¥à¤¯à¥‹à¤‚ à¤®à¥‡à¤‚ à¤¦à¥‡à¤‚, à¤¬à¤¹à¥à¤¤ à¤²à¤‚à¤¬à¤¾ à¤¨ à¤•à¤°à¥‡à¤‚à¥¤
"""
        
        try:
            # Prepare request for Ollama
            ollama_request = {
                "model": self.model_name,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "stream": False,
                "options": {
                    "temperature": 0.7,  # Balanced creativity
                    "top_p": 0.9,
                    "max_tokens": 200,   # Keep responses concise
                    "stop": ["\n\n", "à¤•à¤¿à¤¸à¤¾à¤¨:", "à¤¸à¤µà¤¾à¤²:"]
                }
            }
            
            # Make request to Ollama
            start_time = time.time()
            response = requests.post(
                f"{self.ollama_url}/api/chat",
                json=ollama_request,
                timeout=30
            )
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()
                llm_response = result["message"]["content"].strip()
                
                # Clean up response
                llm_response = self.clean_llm_response(llm_response)
                
                print(f"ğŸ¤– LLM Response Time: {response_time:.2f}s")
                return llm_response
            else:
                print(f"âŒ LLM API Error: {response.status_code}")
                return self.get_fallback_response(intent, entities)
                
        except Exception as e:
            print(f"âŒ LLM Error: {e}")
            return self.get_fallback_response(intent, entities)
    
    def clean_llm_response(self, response: str) -> str:
        """Clean and format LLM response"""
        # Remove unwanted prefixes/suffixes
        unwanted_prefixes = [
            "à¤•à¤¿à¤¸à¤¾à¤¨ à¤œà¥€,", "à¤­à¤¾à¤ˆ à¤¸à¤¾à¤¹à¤¬,", "à¤œà¥€ à¤¹à¤¾à¤,", "à¤¦à¥‡à¤–à¤¿à¤,", 
            "à¤†à¤ªà¤•à¥‹ à¤¬à¤¤à¤¾à¤¨à¤¾ à¤šà¤¾à¤¹à¥‚à¤‚à¤—à¤¾ à¤•à¤¿", "à¤®à¥‡à¤°à¥€ à¤¸à¤²à¤¾à¤¹ à¤¹à¥ˆ à¤•à¤¿"
        ]
        
        for prefix in unwanted_prefixes:
            if response.startswith(prefix):
                response = response[len(prefix):].strip()
        
        # Ensure proper Hindi formatting
        response = response.replace("à¥¤à¥¤", "à¥¤")
        response = response.strip()
        
        # Add appropriate ending if missing
        if not response.endswith(('à¥¤', 'à¥¤', '!')):
            response += "à¥¤"
        
        return response
    
    def get_fallback_response(self, intent: str, entities: Dict) -> str:
        """Provide fallback response when LLM fails"""
        fallback_responses = {
            "seed_inquiry": "à¤¬à¥€à¤œ à¤•à¥€ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤•à¥‡ à¤²à¤¿à¤ à¤¨à¤œà¤¦à¥€à¤•à¥€ à¤•à¥ƒà¤·à¤¿ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤¯à¤¾ à¤¬à¥€à¤œ à¤­à¤‚à¤¡à¤¾à¤° à¤¸à¥‡ à¤¸à¤‚à¤ªà¤°à¥à¤• à¤•à¤°à¥‡à¤‚à¥¤ à¤…à¤šà¥à¤›à¥€ à¤•à¤¿à¤¸à¥à¤® à¤•à¥‡ à¤ªà¥à¤°à¤®à¤¾à¤£à¤¿à¤¤ à¤¬à¥€à¤œ à¤¹à¥€ à¤–à¤°à¥€à¤¦à¥‡à¤‚à¥¤",
            
            "fertilizer_advice": "à¤®à¤¿à¤Ÿà¥à¤Ÿà¥€ à¤ªà¤°à¥€à¤•à¥à¤·à¤£ à¤•à¤°à¤¾à¤•à¤° à¤‰à¤¸à¤•à¥‡ à¤…à¤¨à¥à¤¸à¤¾à¤° à¤¸à¤‚à¤¤à¥à¤²à¤¿à¤¤ à¤‰à¤°à¥à¤µà¤°à¤• à¤•à¤¾ à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚à¥¤ NPK à¤…à¤¨à¥à¤ªà¤¾à¤¤ à¤•à¤¾ à¤§à¥à¤¯à¤¾à¤¨ à¤°à¤–à¥‡à¤‚à¥¤",
            
            "crop_disease": "à¤«à¤¸à¤² à¤®à¥‡à¤‚ à¤°à¥‹à¤— à¤•à¥‡ à¤²à¤•à¥à¤·à¤£ à¤¦à¤¿à¤–à¤¨à¥‡ à¤ªà¤° à¤¤à¥à¤°à¤‚à¤¤ à¤•à¥ƒà¤·à¤¿ à¤µà¤¿à¤¶à¥‡à¤·à¤œà¥à¤ à¤¸à¥‡ à¤¸à¤‚à¤ªà¤°à¥à¤• à¤•à¤°à¥‡à¤‚à¥¤ à¤¸à¤¹à¥€ à¤¦à¤µà¤¾à¤ˆ à¤•à¤¾ à¤›à¤¿à¤¡à¤¼à¤•à¤¾à¤µ à¤•à¤°à¥‡à¤‚à¥¤",
            
            "market_price": "à¤®à¤‚à¤¡à¥€ à¤­à¤¾à¤µ à¤•à¥€ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤•à¥‡ à¤²à¤¿à¤ eNAM à¤ªà¥‹à¤°à¥à¤Ÿà¤² à¤¦à¥‡à¤–à¥‡à¤‚ à¤¯à¤¾ à¤¸à¥à¤¥à¤¾à¤¨à¥€à¤¯ à¤®à¤‚à¤¡à¥€ à¤¸à¥‡ à¤¸à¤‚à¤ªà¤°à¥à¤• à¤•à¤°à¥‡à¤‚à¥¤",
            
            "general": "à¤†à¤ªà¤•à¥€ à¤¸à¤®à¤¸à¥à¤¯à¤¾ à¤•à¥‡ à¤²à¤¿à¤ à¤¨à¤œà¤¦à¥€à¤•à¥€ à¤•à¥ƒà¤·à¤¿ à¤µà¤¿à¤œà¥à¤à¤¾à¤¨ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤¯à¤¾ à¤•à¥ƒà¤·à¤¿ à¤µà¤¿à¤­à¤¾à¤— à¤¸à¥‡ à¤¸à¤‚à¤ªà¤°à¥à¤• à¤•à¤°à¥‡à¤‚à¥¤"
        }
        
        base_response = fallback_responses.get(intent, fallback_responses["general"])
        
        # Add entity-specific information
        if "crops" in entities:
            crops = ", ".join(entities["crops"])
            base_response = f"{crops} à¤•à¥‡ à¤²à¤¿à¤ {base_response}"
        
        return base_response
    
    def process_farmer_query(self, user_query: str) -> Dict:
        """Complete pipeline: NLP â†’ LLM â†’ Response"""
        print(f"\nğŸŒ¾ Processing: {user_query}")
        
        # Step 1: NLP Intent Detection
        print("ğŸ” Step 1: Detecting intent...")
        intent_result = self.nlp_detector.detect_intent(user_query)
        
        intent = intent_result["intent"]
        confidence = intent_result["confidence"]
        entities = intent_result["entities"]
        
        print(f"ğŸ¯ Intent: {intent} (Confidence: {confidence:.2f})")
        if entities:
            print(f"ğŸ·ï¸ Entities: {entities}")
        
        # Step 2: LLM Response Generation
        print("ğŸ¤– Step 2: Generating LLM response...")
        llm_response = self.generate_llm_response(intent_result, user_query)
        
        # Step 3: Build complete result
        result = {
            "user_query": user_query,
            "nlp_result": intent_result,
            "llm_response": llm_response,
            "timestamp": datetime.now().isoformat(),
            "processing_pipeline": "STT â†’ NLP â†’ LLM"
        }
        
        # Add to conversation history
        self.conversation_history.append(result)
        
        return result
    
    def display_response(self, result: Dict):
        """Display formatted response to user"""
        print("\n" + "="*60)
        print("ğŸŒ¾ à¤•à¤¿à¤¸à¤¾à¤¨ à¤¸à¤¹à¤¾à¤¯à¤• à¤•à¤¾ à¤œà¤µà¤¾à¤¬:")
        print("="*60)
        print(f"ğŸ’¬ {result['llm_response']}")
        print("="*60)
        
        # Show technical details
        nlp_result = result["nlp_result"]
        print(f"ğŸ“Š à¤µà¤¿à¤¶à¥à¤µà¤¸à¤¨à¥€à¤¯à¤¤à¤¾: {nlp_result['confidence']:.2f}")
        print(f"ğŸ¯ à¤µà¤¿à¤·à¤¯: {nlp_result['intent']}")
        if nlp_result['entities']:
            print(f"ğŸ·ï¸ à¤ªà¤¹à¤šà¤¾à¤¨à¥€ à¤—à¤ˆ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€: {nlp_result['entities']}")
    
    def get_conversation_summary(self) -> Dict:
        """Get conversation summary"""
        if not self.conversation_history:
            return {"message": "No conversation history"}
        
        total_queries = len(self.conversation_history)
        intents = [entry["nlp_result"]["intent"] for entry in self.conversation_history]
        intent_counts = {}
        
        for intent in intents:
            intent_counts[intent] = intent_counts.get(intent, 0) + 1
        
        avg_confidence = sum(entry["nlp_result"]["confidence"] for entry in self.conversation_history) / total_queries
        
        session_duration = datetime.now() - self.session_start
        
        return {
            "session_duration": str(session_duration),
            "total_queries": total_queries,
            "intent_distribution": intent_counts,
            "average_confidence": avg_confidence,
            "llm_model": self.model_name
        }


def main():
    """Interactive farmer assistant"""
    print("ğŸŒ¾ Farmer LLM Assistant - STT â†’ NLP â†’ LLM Pipeline")
    print("="*70)
    
    try:
        # Initialize assistant
        assistant = FarmerLLMAssistant()
        
        print("\nâœ… System Ready!")
        print("ğŸ’¡ Type your farming questions in Hindi or English")
        print("ğŸ’¡ Type 'quit' to exit")
        print("ğŸ’¡ Type 'summary' to see conversation summary")
        print("-"*70)
        
        while True:
            try:
                # Get user input
                user_input = input("\nğŸ¤ à¤†à¤ªà¤•à¤¾ à¤¸à¤µà¤¾à¤²: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'à¤¬à¤¾à¤¹à¤°', 'à¤¬à¤‚à¤¦']:
                    print("\nğŸ‘‹ à¤§à¤¨à¥à¤¯à¤µà¤¾à¤¦! à¤–à¥‡à¤¤à¥€ à¤®à¥‡à¤‚ à¤¸à¤«à¤²à¤¤à¤¾ à¤•à¥€ à¤¶à¥à¤­à¤•à¤¾à¤®à¤¨à¤¾à¤à¤‚!")
                    break
                
                if user_input.lower() in ['summary', 'à¤¸à¤¾à¤°à¤¾à¤‚à¤¶']:
                    summary = assistant.get_conversation_summary()
                    print("\nğŸ“Š Conversation Summary:")
                    print(json.dumps(summary, indent=2, ensure_ascii=False))
                    continue
                
                if not user_input:
                    print("âš ï¸ à¤•à¥ƒà¤ªà¤¯à¤¾ à¤…à¤ªà¤¨à¤¾ à¤¸à¤µà¤¾à¤² à¤²à¤¿à¤–à¥‡à¤‚à¥¤")
                    continue
                
                # Process query through complete pipeline
                result = assistant.process_farmer_query(user_input)
                
                # Display response
                assistant.display_response(result)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤¬à¤‚à¤¦ à¤•à¤° à¤°à¤¹à¥‡ à¤¹à¥ˆà¤‚...")
                break
            except Exception as e:
                print(f"\nâŒ Error: {e}")
                continue
        
        # Show final summary
        summary = assistant.get_conversation_summary()
        if summary.get("total_queries", 0) > 0:
            print("\nğŸ“ˆ Final Session Summary:")
            print(json.dumps(summary, indent=2, ensure_ascii=False))
            
    except Exception as e:
        print(f"âŒ System initialization failed: {e}")


if __name__ == "__main__":
    main()
